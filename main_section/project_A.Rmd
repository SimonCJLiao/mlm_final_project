---
title: "Project A1+A2 - Model Selection and Notation"
author: "Chongjun Liao, Jeremy Lu"
date: "5/7/2022"
output: pdf_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lme4)
library(lmerTest)
library(dplyr)
library(aod)
```

## A1 Chongjun Liao

0. We will use the classroom.csv data for this project. 
a. math1st will be the outcome of interest for this first part  
i. Recall that `math1st = mathkind + mathgain` 
b. Read in the data (R: store as `dat`) 
c. Fit all models using REML 
d. It’s best if you use `lmerTest::lmer` rather than `lme4::lmer` to call the MLM function. The former provides p-values for fixed effects in the summary. 
e. There are 2 common error messages one can get from lmer calls: failed to converge (problem with hessian: negative eigenvalue; max|grad| = ...); and singularity. They may both be problematic in a real problem, but the latter suggests that a variance component is on the boundary of the parameter space.  
1. In your discussion/writeup, consider the latter to be a “convergence problem” and ignore the former. 
```{r}
dat <- read.csv("~/Documents/GitHub/mlm_final_project/data/classroom.csv")
dat <- dat %>% 
  mutate(math1st = mathkind + mathgain)
```
1. Estimate an Unconditional Means Model (UMM) with random intercepts for both schools and classrooms (nested in schools).  

```{r q1}
fit1 <- lmer( math1st ~ (1 | schoolid/classid), dat)
summary(fit1)
```

a. Report the ICC for schools and the ICC for classrooms  
**Answer:** The ICC for schools is $\frac{\sigma_{\zeta_0}^2}{\sigma_{\zeta_0}^2+\sigma_{\eta_0}^2+\sigma_\varepsilon^2}$=
`r as.data.frame(VarCorr(fit1))$vcov[2]/(as.data.frame(VarCorr(fit1))$vcov[1]+as.data.frame(VarCorr(fit1))$vcov[2]+as.data.frame(VarCorr(fit1))$vcov[3])` and the ICC for classrooms is $\frac{\sigma_{\eta_0}^2}{\sigma_{\zeta_0}^2+\sigma_{\eta_0}^2+\sigma_\varepsilon^2}$=
`r as.data.frame(VarCorr(fit1))$vcov[1]/(as.data.frame(VarCorr(fit1))$vcov[1]+as.data.frame(VarCorr(fit1))$vcov[2]+as.data.frame(VarCorr(fit1))$vcov[3])`.  
b. **WRITE OUT THIS MODEL** using your preferred notation, but use the same choice of notation for the remainder of your project 
i. Be mindful and explicit about any assumptions made.  

$MATH1ST_{ijk} = {b_0} + \zeta_{0k} + \eta _{0jk}+ {\varepsilon _{ijk}}$, with
${\zeta_{0k}}\sim N(0,\sigma_{\zeta_0}^2)$,
$\eta _{0jk} \sim N(0,\sigma_{\eta_0}^2)$ and 
${\varepsilon_{ijk}}\sim N(0,\sigma_\varepsilon^2)$, 
independently of one another, *j*
  represents classrooms and *k*
  represents *schools*.  
2. ADD ALL School level predictors 
```{r q2}
fit2 <- lmer( math1st ~ housepov + (1 | schoolid/classid), dat)
anova(fit1,fit2, refit = T)
wald.test(b = fixef(fit2), Sigma = summary(fit2)$vcov, Terms = 2)
```
a. Report if adding the predictors as a block is justified  
**Answer: **There is only one school-level predictor which is `housepov`, its p-value is `r summary(fit2)$coef["housepov",'Pr(>|t|)']` < 0.05, and I do a LRT on model with and without the school-level predictor, the p-value is `r anova(fit1,fit2)[,'Pr(>Chisq)'][2]` < 0.05. So it is reasonable to add school-level predictor. I also do the wald-test, the p-value is also < 0.05.  

b. Report change in $\sigma_\zeta^2$.  
The change in $\sigma_\zeta^2$ is `r as.data.frame(VarCorr(fit1))$vcov[2]`-`r as.data.frame(VarCorr(fit2))$vcov[2]` = `r as.data.frame(VarCorr(fit1))$vcov[2] - as.data.frame(VarCorr(fit2))$vcov[2]`.  
3. ADD ALL Classroom level predictors  
```{r q3}
fit3 <- lmer( math1st ~ yearstea + mathknow + mathprep + housepov + (1 | schoolid/classid), 
              dat)
summary(fit3)
wald.test(b = fixef(fit3), Sigma = summary(fit3)$vcov, Terms = 2:4)
```

a. Report if adding the predictors as a block is justified [must use WALD test, not LRT]  
**Answer:** The Wald test generates a p-value = 0.32, which shows that we have no reason to add classroom-level predictors as a block. But it might be reasonable to include `mathknow` since it is significant according to the t-test.

b. Report change in $\sigma^2_\eta$ and change in $\sigma^2_\epsilon$.  
**Answer:** The change in $\sigma_\eta^2$ is `r as.data.frame(VarCorr(fit3))$vcov[1]`-`r as.data.frame(VarCorr(fit2))$vcov[1]` = `r as.data.frame(VarCorr(fit3))$vcov[1] - as.data.frame(VarCorr(fit2))$vcov[1]` and change in $\sigma^2_\epsilon$ is `r as.data.frame(VarCorr(fit3))$vcov[3]`-`r as.data.frame(VarCorr(fit2))$vcov[3]` = `r as.data.frame(VarCorr(fit3))$vcov[3] - as.data.frame(VarCorr(fit2))$vcov[3]`.
c. Give a potential reason as to why $\sigma^2_\epsilon$ is reduced, but not $\sigma^2_\eta$?  

One potential reason is that there are only 3~4 sampled student in each classroom. Since the sample size with each classroom is small, the classroom predictors describe aggregrate limited individual characteristics, which would explain student-level variation.

  4. ADD (nearly) ALL student level predictors (but not `mathgain` or `mathkind`, as these are outcomes in this context). 
```{r}
fit4 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                housepov + (1 | schoolid/classid), dat)
summary(fit4)
wald.test(b = fixef(fit4), Sigma = summary(fit4)$vcov, Terms = 2:4)
```

a. Report if justified statistically as a block of predictors [must use WALD test, not LRT]  
**Answer:** The wald test gives a p-value less than 0.05, which justifies the significance of adding a block of individual predictors.  

b. Report change in variance components for all levels  
**Answer:** The change in $\sigma_\eta^2$ is `r as.data.frame(VarCorr(fit4))$vcov[1]`-`r as.data.frame(VarCorr(fit3))$vcov[1]` = `r as.data.frame(VarCorr(fit4))$vcov[1] - as.data.frame(VarCorr(fit3))$vcov[1]`, increases; the change in $\sigma_\zeta^2$ is `r as.data.frame(VarCorr(fit4))$vcov[2]`-`r as.data.frame(VarCorr(fit3))$vcov[2]` = `r as.data.frame(VarCorr(fit4))$vcov[2] - as.data.frame(VarCorr(fit3))$vcov[2]`, decreases; and change in $\sigma^2_\epsilon$ is `r as.data.frame(VarCorr(fit4))$vcov[3]`-`r as.data.frame(VarCorr(fit3))$vcov[3]` = `r as.data.frame(VarCorr(fit4))$vcov[3] - as.data.frame(VarCorr(fit3))$vcov[3]`, decreases.  

c. Give a potential reason as to why the school level variance component drops from prior model  
The aggregate effect of individual predictors account for school-level variance, as a result the school-level  variance component drops.  

d. **WRITE OUT THIS MODEL** using your chosen notation (include assumptions).  

$MATH1ST_{ijk} = {b_0} + b_1SES_{ijk} + b_2MINORITY_{ijk} + b_3SEX_{ijk} + b_4YEARSTEA_{jk} + b_5MATHKNOW_{jk} + b_6MATHPREP_{jk}+ b_7HOUSEPOV_{k} + \zeta _{0k} + \eta _{0jk}+ {\varepsilon _{ijk}}$, with
${\zeta_{0k}}\sim N(0,\sigma_{\zeta_0}^2)$,
$\eta _{0jk} \sim N(0,\sigma_{\eta_0}^2)$ and 
${\varepsilon_{ijk}}\sim N(0,\sigma_\varepsilon^2)$, 
independently of one another, *j*
  represents classrooms and *k*
  represents *schools*.  
  
5.a. Try to add a random slope for each teacher level predictor (varying at the school level; one by one separately- not all together)  

```{r}
fit5.1 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                  housepov + (1 | schoolid/classid) + (0 + yearstea | schoolid), 
                dat)
summary(fit5.1)
```

```{r}
fit5.2 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                  housepov + (1 | schoolid/classid) + (0 + mathknow | schoolid),
                dat)
summary(fit5.2)
```

```{r}
fit5.3 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                  housepov + (1 | schoolid/classid) + (0 + mathprep | schoolid), 
                dat)
summary(fit5.3)
```
b. Report the model fit or lack of fit 
**Answer:** The model with random slope on `mathknow` and the model with random slope on `mathprep` have convergent problem. Besides, all these three random slopes have variation that is close to 0, which indicates that these models are poorly fitted.

c. Retry the above, allowing the slopes to be correlated with the random intercepts (still one by one) 

```{r}
fit5.c.1 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                    housepov + (yearstea | schoolid) + (1 | schoolid:classid), 
                  dat)
summary(fit5.c.1)
```

```{r}
fit5.c.2 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                    housepov + (mathknow| schoolid) + (1 | schoolid:classid), 
                  dat)
summary(fit5.c.2)
```

```{r}
fit5.c.3 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                    housepov + (mathprep | schoolid) + (1 | schoolid:classid), 
                  dat)
summary(fit5.c.3)
```

```{r, echo=FALSE}
# classroom intercept
classroom <- data.frame(five_b = c(VarCorr(fit5.1)[[1]][1,1],
                               VarCorr(fit5.2)[[1]][1,1],
                               VarCorr(fit5.3)[[1]][1,1]),
           five_c = c(VarCorr(fit5.c.1)[[1]][1,1],
                      VarCorr(fit5.c.2)[[1]][1,1],
                      VarCorr(fit5.c.3)[[1]][1,1]))
rownames(classroom) = c("yearstea","mathknow","mathprep")
knitr::kable(round(classroom,3),caption = "variation explained by classroom-level random intercept")
# school intercept
school_intercept <- data.frame(five_b = c(VarCorr(fit5.1)[[2]][1,1],
                                      VarCorr(fit5.2)[[2]][1,1],
                                      VarCorr(fit5.3)[[2]][1,1]),
           five_c = c(VarCorr(fit5.c.1)[[2]][1,1],
                      VarCorr(fit5.c.2)[[2]][1,1],
                      VarCorr(fit5.c.3)[[2]][1,1]))
rownames(school_intercept) = c("yearstea","mathknow","mathprep")
knitr::kable(round(school_intercept,3),caption = "variation explained by school-level random intercept")
# school random slope
school_slope <- data.frame(five_b = c(data.frame(VarCorr(fit5.1))[3,4], 
                                  data.frame(VarCorr(fit5.2))[3,4], 
                                  data.frame(VarCorr(fit5.3))[3,4]),
                       five_c = c(data.frame(VarCorr(fit5.c.1))[3,4], 
                                  data.frame(VarCorr(fit5.c.2))[3,4], 
                                  data.frame(VarCorr(fit5.c.3))[3,4]))
rownames(school_slope) = c("yearstea","mathknow","mathprep")
knitr::kable(round(school_slope,3),caption = "variation explained by school-level random slope")
```

d. Report anything unusual about the variance components (changes that are in a direction you didn’t expect) and any potential explanation for why those changes occurred (hint: what did you add to the model?).  

**Answer: ** For `mathknow`, the variation of school-level random slope increase while variation of school-level random intercept decrease. For `yearstea` and `mathprep`, both school-level random slope and school-level random intercept increase variation. Potential reason is that random slope on `mathknow` and random intercept are positively correlated, to explain same amount of school-level variation, the decrease in variation of school-level random intercept would be compensated by the positive covariance. Similarly for `yearstea` and `mathprep` the increase in variance of random slope and random intercept would be compensated by the negative covariance.
6. Question: 
  a. Why is it a bad idea to include a classroom-level variable with random slopes at the classroom level?  
**Answer: **Classroom-level variables does not vary within classroom, if there is no variation on variable, the slope could not be measured, so adding a random slope on classroom variable at classroom level makes no sense.

## A2 Jeremy Lu

7. Question:
  a. For UMM, write down: V_S, V_C, V_E for the three variance components (simply the estimates)  
 **Answer:**  We have that V_S = 280.68, V_C = 85.46, and V_E = 1146.8  
  b. For the most complicated (all fixed effects) random INTERCEPTS ONLY model, what are: V_C, V_S, V_E?  
  **Answer:** We have in this model that V_S = 169.45, V_C = 93.89, V_E = 1064.96  
  c. By what fraction did these each decrease with the new predictors in the model?  
  **Answer:** The fraction decrease for V_S, and V_E are `r round((280.68-169.45)/280.68, 3)`, and `r round((1146.8-1064.96)/1146.8, 3)`, respectively. But for V_C it actually increased `r round((93.89-85.46)/85.46, 3)` fraction-wise.  
  
8. a. 
```{r}
fit8.a.1 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                housepov + (1 | schoolid/classid) + (0 + ses | schoolid), 
                dat)
summary(fit8.a.1)
```

```{r}
fit8.a.2 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                housepov + (1 | schoolid/classid) + (0 + sex | schoolid), 
                dat)
summary(fit8.a.2)
```

```{r}
fit8.a.3 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                housepov + (1 | schoolid/classid) + (0 + minority | schoolid), 
                dat)
summary(fit8.a.3)
```

b. Retry part (a), allowing the slopes to be correlated with the random intercepts.

```{r}
fit8.b.1 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                housepov + (1 | classid) + (ses | schoolid), dat)
summary(fit8.b.1)
```

```{r}
fit8.b.2 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                housepov + (1 | schoolid:classid) + (sex | schoolid), dat)
summary(fit8.b.2)
```

```{r}
fit8.b.3 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                housepov + (1 | classid) + (minority | schoolid), dat)
summary(fit8.b.3)
```

c. Report anything unusual about the variance components (changes that are unexpected)   

**Answer:**  Adding the correlation between school-level random slope on any of these student-level predictors, and the school-level random intercept, both the variations captured by the school-level random slope and the variation of random intercept increase substantially, especially for adding correlation between random slope on `minority` and random intercept.  

9. a. Take the two predictors that had significant (at .05 level) random slopes, in the forms in which they worked (indep. or correlated) and add both to the model, and test for need of one conditional on already including the other.  
```{r}
# check significance of random slope
anova(fit8.a.1,fit4,refit=F)
anova(fit8.b.1,fit4,refit=F)
anova(fit8.a.2,fit4,refit=F)
anova(fit8.b.2,fit4,refit=F)
anova(fit8.a.3,fit4,refit=F)
anova(fit8.b.3,fit4,refit=F)
```

```{r}
# random slope of ses without correlation and random slope of minority with 
# correlation are significant.
fit9 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                housepov + (1 | classid:schoolid) + (0 + ses | schoolid) + 
                (minority | schoolid), dat)
anova(fit8.a.1, fit9,refit=F) #P = 0.00204
anova(fit8.b.3, fit9,refit=F) #P = 0.02365
```
  b. Is the more complex model (with both random slopes in it) justified?  
**Answer:** Yes, both random slopes are significant according to the LRT.
  c. WRITE OUT THIS MODEL in your preferred notation (include assumptions)  
$MATH1ST_{ijk} = {b_0} + (b_1+\zeta_{1k})SES_{ijk} +( b_2+\zeta_{2k})MINORITY_{ijk} + b_3SEX_{ijk} + b_4YEARSTEA_{jk} + b_5MATHKNOW_{jk} + b_6MATHPREP_{jk}+ b_7HOUSEPOV_{k} + \zeta _{0k} + \eta _{0jk}+ {\varepsilon _{ijk}}$, with
${\zeta_{0k}}\sim N(0,\sigma_{\zeta_0}^2)$, ${\zeta_{1k}}\sim N(0,\sigma_{\zeta_1}^2)$, ${\zeta_{3k}}\sim N(0,\sigma_{\zeta_3}^2)$
$\eta _{0jk} \sim N(0,\sigma_{\eta_0}^2)$ and 
${\varepsilon_{ijk}}\sim N(0,\sigma_\varepsilon^2)$,
$corr(\zeta_{0k},\zeta_{1k}) = 0$, and 
$corr(\zeta_{0k},\zeta_{2k}) = 0$, the other random components are independent of each others.

10. Now consider the model with a random slope *only* in minority. We will make predictions at levels of minority in the range 0 to 1 for illustrative purposes.
  a. What are: V_C, V_S(minority=0), V_E?
    i. We need to list 'minority=0' here, or we don't know how to use the slope variance.
**Answer: ** V_C = 86.69, V_S = 381.20, V_E = 1039.39
```{r}
data.frame(VarCorr(fit8.b.3))
```

  b. What are: V_S(minority=0.25), V_S(minority=+0.50), V_S(minority=+0.75)?  
```{r}
# minority = 0.25
paste0("V_S(minority=0.25) =", data.frame(VarCorr(fit8.b.3))[2,4]+0.25^2 * 
         data.frame(VarCorr(fit8.b.3))[3,4] + 0.25*2*
         data.frame(VarCorr(fit8.b.3))[4,5]*data.frame(VarCorr(fit8.b.3))[3,5]*
         data.frame(VarCorr(fit8.b.3))[2,5])
# minority = 0.5
paste0("V_S(minority=0.5) =", data.frame(VarCorr(fit8.b.3))[2,4]+0.5^2 * 
         data.frame(VarCorr(fit8.b.3))[3,4] + 0.5*2*
         data.frame(VarCorr(fit8.b.3))[4,5]*data.frame(VarCorr(fit8.b.3))[3,5]*
         data.frame(VarCorr(fit8.b.3))[2,5])
# minority = 0.75
paste0("V_S(minority=0.75) =", data.frame(VarCorr(fit8.b.3))[2,4]+0.75^2 * 
         data.frame(VarCorr(fit8.b.3))[3,4] + 0.75*2*
         data.frame(VarCorr(fit8.b.3))[4,5]*data.frame(VarCorr(fit8.b.3))[3,5]*
         data.frame(VarCorr(fit8.b.3))[2,5])
```

**Answer: **  
V_S(minority=0.25) = $\sigma_{\zeta_{0k}^2}+2 \times0.25 \times\rho_{\zeta_{0k},\zeta_{2k}} +0.25^2\sigma_{\zeta_{2k}^2}$ =
`r data.frame(VarCorr(fit8.b.3))[2,4]+0.25^2 * data.frame(VarCorr(fit8.b.3))[3,4] + 0.5*data.frame(VarCorr(fit8.b.3))[4,5]*data.frame(VarCorr(fit8.b.3))[3,5]*data.frame(VarCorr(fit8.b.3))[2,5]`,  
V_S(minority=0.50) =$\sigma_{\zeta_{0k}^2}+2 \times0.5 \times\rho_{\zeta_{0k},\zeta_{2k}}+0.5^2\sigma_{\zeta_{2k}^2}$= 
`r data.frame(VarCorr(fit8.b.3))[2,4]+0.5^2*data.frame(VarCorr(fit8.b.3))[3,4]+data.frame(VarCorr(fit8.b.3))[4,5]*data.frame(VarCorr(fit8.b.3))[3,5]*data.frame(VarCorr(fit8.b.3))[2,5]`,  
V_S(minority=0.75) = $\sigma_{\zeta_{0k}^2}+2 \times0.75 \times\rho_{\zeta_{0k},\zeta_{2k}}+0.75^2\sigma_{\zeta_{2k}^2}$=
`r data.frame(VarCorr(fit8.b.3))[2,4]+0.75^2 * data.frame(VarCorr(fit8.b.3))[3,4] + 1.5*data.frame(VarCorr(fit8.b.3))[4,5]*data.frame(VarCorr(fit8.b.3))[3,5]*data.frame(VarCorr(fit8.b.3))[2,5]`  
  c. Is the variance between schools monotonically *increasing* in the value of minority?  
**Answer: ** No, it seems to be decreasing from minority 0 to 0.75 given the variance calculated.