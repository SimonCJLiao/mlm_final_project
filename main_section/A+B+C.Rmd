---
title: "Project A1+A2+B+C"
author: "Chongjun Liao, Jeremy Lu, Yu Wang, Xinming Dai"
date: "5/7/2022"
output: 
  pdf_document:
    toc: true
    toc_depth: 2
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lme4)
library(lmerTest)
library(dplyr)
library(aod)
```

# A1, Chongjun Liao

0. We will use the classroom.csv data for this project. 
a. math1st will be the outcome of interest for this first part  
i. Recall that `math1st = mathkind + mathgain` 
b. Read in the data (R: store as `dat`) 
c. Fit all models using REML 
d. It’s best if you use `lmerTest::lmer` rather than `lme4::lmer` to call the MLM function. The former provides p-values for fixed effects in the summary. 
e. There are 2 common error messages one can get from lmer calls: failed to converge (problem with hessian: negative eigenvalue; max|grad| = ...); and singularity. They may both be problematic in a real problem, but the latter suggests that a variance component is on the boundary of the parameter space.  
1. In your discussion/writeup, consider the latter to be a “convergence problem” and ignore the former. 
```{r}
dat <- read.csv("~/Documents/GitHub/mlm_final_project/data/classroom.csv")
dat <- dat %>% 
  mutate(math1st = mathkind + mathgain)
```
1. Estimate an Unconditional Means Model (UMM) with random intercepts for both schools and classrooms (nested in schools).  

```{r q1}
fit1 <- lmer( math1st ~ (1 | schoolid/classid), dat)
summary(fit1)
```

a. Report the ICC for schools and the ICC for classrooms  
**Answer:** The ICC for schools is $\frac{\sigma_{\zeta_0}^2}{\sigma_{\zeta_0}^2+\sigma_{\eta_0}^2+\sigma_\varepsilon^2}$=
`r as.data.frame(VarCorr(fit1))$vcov[2]/(as.data.frame(VarCorr(fit1))$vcov[1]+as.data.frame(VarCorr(fit1))$vcov[2]+as.data.frame(VarCorr(fit1))$vcov[3])` and the ICC for classrooms is $\frac{\sigma_{\eta_0}^2}{\sigma_{\zeta_0}^2+\sigma_{\eta_0}^2+\sigma_\varepsilon^2}$=
`r as.data.frame(VarCorr(fit1))$vcov[1]/(as.data.frame(VarCorr(fit1))$vcov[1]+as.data.frame(VarCorr(fit1))$vcov[2]+as.data.frame(VarCorr(fit1))$vcov[3])`.  
b. **WRITE OUT THIS MODEL** using your preferred notation, but use the same choice of notation for the remainder of your project 
i. Be mindful and explicit about any assumptions made.  

$MATH1ST_{ijk} = {b_0} + \zeta_{0k} + \eta _{0jk}+ {\varepsilon _{ijk}}$, with
${\zeta_{0k}}\sim N(0,\sigma_{\zeta_0}^2)$,
$\eta _{0jk} \sim N(0,\sigma_{\eta_0}^2)$ and 
${\varepsilon_{ijk}}\sim N(0,\sigma_\varepsilon^2)$, 
independently of one another, *j*
  represents classrooms and *k*
  represents *schools*.  
2. ADD ALL School level predictors 
```{r q2}
fit2 <- lmer( math1st ~ housepov + (1 | schoolid/classid), dat)
anova(fit1,fit2, refit = T)
wald.test(b = fixef(fit2), Sigma = summary(fit2)$vcov, Terms = 2)
```
a. Report if adding the predictors as a block is justified  
**Answer: **There is only one school-level predictor which is `housepov`, its p-value is `r summary(fit2)$coef["housepov",'Pr(>|t|)']` < 0.05, and I do a LRT on model with and without the school-level predictor, the p-value is `r anova(fit1,fit2)[,'Pr(>Chisq)'][2]` < 0.05. So it is reasonable to add school-level predictor. I also do the wald-test, the p-value is also < 0.05.  

b. Report change in $\sigma_\zeta^2$.  
The change in $\sigma_\zeta^2$ is `r as.data.frame(VarCorr(fit1))$vcov[2]`-`r as.data.frame(VarCorr(fit2))$vcov[2]` = `r as.data.frame(VarCorr(fit1))$vcov[2] - as.data.frame(VarCorr(fit2))$vcov[2]`.  
3. ADD ALL Classroom level predictors  
```{r q3}
fit3 <- lmer( math1st ~ yearstea + mathknow + mathprep + housepov + (1 | schoolid/classid), 
              dat)
summary(fit3)
wald.test(b = fixef(fit3), Sigma = summary(fit3)$vcov, Terms = 2:4)
```

a. Report if adding the predictors as a block is justified [must use WALD test, not LRT]  
**Answer:** The Wald test generates a p-value = 0.32, which shows that we have no reason to add classroom-level predictors as a block. But it might be reasonable to include `mathknow` since it is significant according to the t-test.

b. Report change in $\sigma^2_\eta$ and change in $\sigma^2_\epsilon$.  
**Answer:** The change in $\sigma_\eta^2$ is `r as.data.frame(VarCorr(fit3))$vcov[1]`-`r as.data.frame(VarCorr(fit2))$vcov[1]` = `r as.data.frame(VarCorr(fit3))$vcov[1] - as.data.frame(VarCorr(fit2))$vcov[1]` and change in $\sigma^2_\epsilon$ is `r as.data.frame(VarCorr(fit3))$vcov[3]`-`r as.data.frame(VarCorr(fit2))$vcov[3]` = `r as.data.frame(VarCorr(fit3))$vcov[3] - as.data.frame(VarCorr(fit2))$vcov[3]`.
c. Give a potential reason as to why $\sigma^2_\epsilon$ is reduced, but not $\sigma^2_\eta$?  

One potential reason is that there are only 3~4 sampled student in each classroom. Since the sample size with each classroom is small, the classroom predictors describe aggregrate limited individual characteristics, which would explain student-level variation.

  4. ADD (nearly) ALL student level predictors (but not `mathgain` or `mathkind`, as these are outcomes in this context). 
```{r}
fit4 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                housepov + (1 | schoolid/classid), dat)
summary(fit4)
wald.test(b = fixef(fit4), Sigma = summary(fit4)$vcov, Terms = 2:4)
```

a. Report if justified statistically as a block of predictors [must use WALD test, not LRT]  
**Answer:** The wald test gives a p-value less than 0.05, which justifies the significance of adding a block of individual predictors.  

b. Report change in variance components for all levels  
**Answer:** The change in $\sigma_\eta^2$ is `r as.data.frame(VarCorr(fit4))$vcov[1]`-`r as.data.frame(VarCorr(fit3))$vcov[1]` = `r as.data.frame(VarCorr(fit4))$vcov[1] - as.data.frame(VarCorr(fit3))$vcov[1]`, increases; the change in $\sigma_\zeta^2$ is `r as.data.frame(VarCorr(fit4))$vcov[2]`-`r as.data.frame(VarCorr(fit3))$vcov[2]` = `r as.data.frame(VarCorr(fit4))$vcov[2] - as.data.frame(VarCorr(fit3))$vcov[2]`, decreases; and change in $\sigma^2_\epsilon$ is `r as.data.frame(VarCorr(fit4))$vcov[3]`-`r as.data.frame(VarCorr(fit3))$vcov[3]` = `r as.data.frame(VarCorr(fit4))$vcov[3] - as.data.frame(VarCorr(fit3))$vcov[3]`, decreases.  

c. Give a potential reason as to why the school level variance component drops from prior model  
The aggregate effect of student predictors, can be seen as the school-level means and student deviation from the school mean. The school means would account for school-level variance, as a result the school-level variance component drops.  

d. **WRITE OUT THIS MODEL** using your chosen notation (include assumptions).  

$MATH1ST_{ijk} = {b_0} + b_1SES_{ijk} + b_2MINORITY_{ijk} + b_3SEX_{ijk} + b_4YEARSTEA_{jk} + b_5MATHKNOW_{jk} + b_6MATHPREP_{jk}+ b_7HOUSEPOV_{k} + \zeta _{0k} + \eta _{0jk}+ {\varepsilon _{ijk}}$, with
${\zeta_{0k}}\sim N(0,\sigma_{\zeta_0}^2)$,
$\eta _{0jk} \sim N(0,\sigma_{\eta_0}^2)$ and 
${\varepsilon_{ijk}}\sim N(0,\sigma_\varepsilon^2)$, 
independently of one another, *j*
  represents classrooms and *k*
  represents *schools*.  
  
5.a. Try to add a random slope for each teacher level predictor (varying at the school level; one by one separately- not all together)  

```{r}
fit5.1 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                  housepov + (1 | schoolid/classid) + (0 + yearstea | schoolid), 
                dat)
summary(fit5.1)
```

```{r}
fit5.2 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                  housepov + (1 | schoolid/classid) + (0 + mathknow | schoolid),
                dat)
summary(fit5.2)
```

```{r}
fit5.3 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                  housepov + (1 | schoolid/classid) + (0 + mathprep | schoolid), 
                dat)
summary(fit5.3)
```
b. Report the model fit or lack of fit 
**Answer:** The model with random slope on `mathknow` and the model with random slope on `mathprep` have convergent problem. Besides, all these three random slopes have variation that is close to 0, which indicates that these models are poorly fitted.

c. Retry the above, allowing the slopes to be correlated with the random intercepts (still one by one) 

```{r}
fit5.c.1 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                    housepov + (yearstea | schoolid) + (1 | schoolid:classid), 
                  dat)
summary(fit5.c.1)
```

```{r}
fit5.c.2 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                    housepov + (mathknow| schoolid) + (1 | schoolid:classid), 
                  dat)
summary(fit5.c.2)
```

```{r}
fit5.c.3 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                    housepov + (mathprep | schoolid) + (1 | schoolid:classid), 
                  dat)
summary(fit5.c.3)
```

```{r, echo=FALSE}
# classroom intercept
classroom <- data.frame(five_b = c(VarCorr(fit5.1)[[1]][1,1],
                               VarCorr(fit5.2)[[1]][1,1],
                               VarCorr(fit5.3)[[1]][1,1]),
           five_c = c(VarCorr(fit5.c.1)[[1]][1,1],
                      VarCorr(fit5.c.2)[[1]][1,1],
                      VarCorr(fit5.c.3)[[1]][1,1]))
rownames(classroom) = c("yearstea","mathknow","mathprep")
knitr::kable(round(classroom,3),caption = "variation explained by classroom-level random intercept")
# school intercept
school_intercept <- data.frame(five_b = c(VarCorr(fit5.1)[[2]][1,1],
                                      VarCorr(fit5.2)[[2]][1,1],
                                      VarCorr(fit5.3)[[2]][1,1]),
           five_c = c(VarCorr(fit5.c.1)[[2]][1,1],
                      VarCorr(fit5.c.2)[[2]][1,1],
                      VarCorr(fit5.c.3)[[2]][1,1]))
rownames(school_intercept) = c("yearstea","mathknow","mathprep")
knitr::kable(round(school_intercept,3),caption = "variation explained by school-level random intercept")
# school random slope
school_slope <- data.frame(five_b = c(data.frame(VarCorr(fit5.1))[3,4], 
                                  data.frame(VarCorr(fit5.2))[3,4], 
                                  data.frame(VarCorr(fit5.3))[3,4]),
                       five_c = c(data.frame(VarCorr(fit5.c.1))[3,4], 
                                  data.frame(VarCorr(fit5.c.2))[3,4], 
                                  data.frame(VarCorr(fit5.c.3))[3,4]))
rownames(school_slope) = c("yearstea","mathknow","mathprep")
knitr::kable(round(school_slope,3),caption = "variation explained by school-level random slope")
```

d. Report anything unusual about the variance components (changes that are in a direction you didn’t expect) and any potential explanation for why those changes occurred (hint: what did you add to the model?).  

**Answer: ** For `mathknow`, the variation of school-level random slope increase while variation of school-level random intercept decrease. For `yearstea` and `mathprep`, both school-level random slope and school-level random intercept increase variation. Potential reason is that random slope on `mathknow` and random intercept are positively correlated, to explain same amount of school-level variation, the decrease in variation of school-level random intercept would be compensated by the positive covariance. Similarly for `yearstea` and `mathprep` the increase in variance of random slope and random intercept would be compensated by the negative covariance.
6. Question: 
  a. Why is it a bad idea to include a classroom-level variable with random slopes at the classroom level?  
**Answer: **Classroom-level variables does not vary within classroom, if there is no variation on variable, the slope could not be measured, so adding a random slope on classroom variable at classroom level makes no sense.

# A2, Jeremy Lu

7. Question:
  a. For UMM, write down: V_S, V_C, V_E for the three variance components (simply the estimates)  
 **Answer:**  We have that V_S = 280.68, V_C = 85.46, and V_E = 1146.8  
  b. For the most complicated (all fixed effects) random INTERCEPTS ONLY model, what are: V_C, V_S, V_E?  
  **Answer:** We have in this model that V_S = 169.45, V_C = 93.89, V_E = 1064.96  
  c. By what fraction did these each decrease with the new predictors in the model?  
  **Answer:** The fraction decrease for V_S, and V_E are `r round((280.68-169.45)/280.68, 3)`, and `r round((1146.8-1064.96)/1146.8, 3)`, respectively. But for V_C it actually increased `r round((93.89-85.46)/85.46, 3)` fraction-wise.  
  
8. a. 
```{r}
fit8.a.1 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                housepov + (1 | schoolid/classid) + (0 + ses | schoolid), 
                dat)
summary(fit8.a.1)
```

```{r}
fit8.a.2 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                housepov + (1 | schoolid/classid) + (0 + sex | schoolid), 
                dat)
summary(fit8.a.2)
```

```{r}
fit8.a.3 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                housepov + (1 | schoolid/classid) + (0 + minority | schoolid), 
                dat)
summary(fit8.a.3)
```

b. Retry part (a), allowing the slopes to be correlated with the random intercepts.

```{r}
fit8.b.1 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                housepov + (1 | classid) + (ses | schoolid), dat)
summary(fit8.b.1)
```

```{r}
fit8.b.2 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                housepov + (1 | schoolid:classid) + (sex | schoolid), dat)
summary(fit8.b.2)
```

```{r}
fit8.b.3 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                housepov + (1 | classid) + (minority | schoolid), dat)
summary(fit8.b.3)
```

c. Report anything unusual about the variance components (changes that are unexpected)   

**Answer:**  Adding the correlation between school-level random slope on any of these student-level predictors, and the school-level random intercept, both the variations captured by the school-level random slope and the variation of random intercept increase substantially, especially for adding correlation between random slope on `minority` and random intercept.  

9. a. Take the two predictors that had significant (at .05 level) random slopes, in the forms in which they worked (indep. or correlated) and add both to the model, and test for need of one conditional on already including the other.  
```{r}
# check significance of random slope
anova(fit8.a.1,fit4,refit=F)
anova(fit8.b.1,fit4,refit=F)
anova(fit8.a.2,fit4,refit=F)
anova(fit8.b.2,fit4,refit=F)
anova(fit8.a.3,fit4,refit=F)
anova(fit8.b.3,fit4,refit=F)
```

```{r}
# random slope of ses without correlation and random slope of minority with 
# correlation are significant.
fit9 <- lmer( math1st ~ ses + minority + sex + yearstea + mathknow + mathprep + 
                housepov + (1 | classid:schoolid) + (0 + ses | schoolid) + 
                (minority | schoolid), dat)
anova(fit8.a.1, fit9,refit=F) #P = 0.00204
anova(fit8.b.3, fit9,refit=F) #P = 0.02365
```
  b. Is the more complex model (with both random slopes in it) justified?  
**Answer:** Yes, both random slopes are significant according to the LRT.
  c. WRITE OUT THIS MODEL in your preferred notation (include assumptions)  
$MATH1ST_{ijk} = {b_0} + (b_1+\zeta_{1k})SES_{ijk} +( b_2+\zeta_{2k})MINORITY_{ijk} + b_3SEX_{ijk} + b_4YEARSTEA_{jk} + b_5MATHKNOW_{jk} + b_6MATHPREP_{jk}+ b_7HOUSEPOV_{k} + \zeta _{0k} + \eta _{0jk}+ {\varepsilon _{ijk}}$, with
${\zeta_{0k}}\sim N(0,\sigma_{\zeta_0}^2)$, ${\zeta_{1k}}\sim N(0,\sigma_{\zeta_1}^2)$, ${\zeta_{3k}}\sim N(0,\sigma_{\zeta_3}^2)$
$\eta _{0jk} \sim N(0,\sigma_{\eta_0}^2)$ and 
${\varepsilon_{ijk}}\sim N(0,\sigma_\varepsilon^2)$,
$corr(\zeta_{0k},\zeta_{1k}) = 0$, and 
$corr(\zeta_{0k},\zeta_{2k}) = 0$, the other random components are independent of each others.

10. Now consider the model with a random slope *only* in minority. We will make predictions at levels of minority in the range 0 to 1 for illustrative purposes.
  a. What are: V_C, V_S(minority=0), V_E?
    i. We need to list 'minority=0' here, or we don't know how to use the slope variance.
**Answer: ** V_C = 86.69, V_S = 381.20, V_E = 1039.39
```{r}
data.frame(VarCorr(fit8.b.3))
```

  b. What are: V_S(minority=0.25), V_S(minority=+0.50), V_S(minority=+0.75)?  
```{r}
# minority = 0.25
paste0("V_S(minority=0.25) =", data.frame(VarCorr(fit8.b.3))[2,4]+0.25^2 * 
         data.frame(VarCorr(fit8.b.3))[3,4] + 0.25*2*
         data.frame(VarCorr(fit8.b.3))[4,5]*data.frame(VarCorr(fit8.b.3))[3,5]*
         data.frame(VarCorr(fit8.b.3))[2,5])
# minority = 0.5
paste0("V_S(minority=0.5) =", data.frame(VarCorr(fit8.b.3))[2,4]+0.5^2 * 
         data.frame(VarCorr(fit8.b.3))[3,4] + 0.5*2*
         data.frame(VarCorr(fit8.b.3))[4,5]*data.frame(VarCorr(fit8.b.3))[3,5]*
         data.frame(VarCorr(fit8.b.3))[2,5])
# minority = 0.75
paste0("V_S(minority=0.75) =", data.frame(VarCorr(fit8.b.3))[2,4]+0.75^2 * 
         data.frame(VarCorr(fit8.b.3))[3,4] + 0.75*2*
         data.frame(VarCorr(fit8.b.3))[4,5]*data.frame(VarCorr(fit8.b.3))[3,5]*
         data.frame(VarCorr(fit8.b.3))[2,5])
```

**Answer: **  
V_S(minority=0.25) = $\sigma_{\zeta_{0k}^2}+2 \times0.25 \times\rho_{\zeta_{0k},\zeta_{2k}} +0.25^2\sigma_{\zeta_{2k}^2}$ =
`r data.frame(VarCorr(fit8.b.3))[2,4]+0.25^2 * data.frame(VarCorr(fit8.b.3))[3,4] + 0.5*data.frame(VarCorr(fit8.b.3))[4,5]*data.frame(VarCorr(fit8.b.3))[3,5]*data.frame(VarCorr(fit8.b.3))[2,5]`,  
V_S(minority=0.50) =$\sigma_{\zeta_{0k}^2}+2 \times0.5 \times\rho_{\zeta_{0k},\zeta_{2k}}+0.5^2\sigma_{\zeta_{2k}^2}$= 
`r data.frame(VarCorr(fit8.b.3))[2,4]+0.5^2*data.frame(VarCorr(fit8.b.3))[3,4]+data.frame(VarCorr(fit8.b.3))[4,5]*data.frame(VarCorr(fit8.b.3))[3,5]*data.frame(VarCorr(fit8.b.3))[2,5]`,  
V_S(minority=0.75) = $\sigma_{\zeta_{0k}^2}+2 \times0.75 \times\rho_{\zeta_{0k},\zeta_{2k}}+0.75^2\sigma_{\zeta_{2k}^2}$=
`r data.frame(VarCorr(fit8.b.3))[2,4]+0.75^2 * data.frame(VarCorr(fit8.b.3))[3,4] + 1.5*data.frame(VarCorr(fit8.b.3))[4,5]*data.frame(VarCorr(fit8.b.3))[3,5]*data.frame(VarCorr(fit8.b.3))[2,5]`  
  c. Is the variance between schools monotonically *increasing* in the value of minority?  
**Answer: ** No, it seems to be decreasing from minority 0 to 0.75 given the variance calculated.  

# Project B, Xinming Dai

## Question 0: read data and process missingness

```{r question 0}
dat <- read.csv("classroom.csv")

# construct new outcome math1st
dat <- 
  dat %>% 
  mutate(math1st = mathkind + mathgain)

# remove missing data
dat <- 
  dat %>% 
  filter(complete.cases(dat))
```

## Question 1
```{r question 1}
# fit a model
fit1 <- lmerTest::lmer(math1st ~ housepov + yearstea + mathprep + mathknow + ses + sex + minority + (1|schoolid/classid), data = dat)
summary(fit1)

# plot residuals to test normality assumption
res1 <- residuals(fit1)
# density plot
plot(density(res1))
# QQ plot
qqnorm(res1, pch = 1, frame = FALSE)
qqline(res1, col = "steelblue", lwd = 2)
```

QQ plot shows that points are around the line, and thus we believe the normality assumption holds.

## Question 2

```{r question 2}
# Generate the two sets of BLUPs (for random effects zeta0 and eta0)
blups_fit1 <- ranef(fit1)

par(mfrow=c(2,2))  
# examine normality for eta0 (class-level)
eta0_fit1 <- blups_fit1$`classid:schoolid`$`(Intercept)`
# density plot
plot(density(eta0_fit1))
# QQ plot
qqnorm(eta0_fit1, pch = 1, frame = FALSE, main = "Normal Q-Q plot for eta0")
qqline(eta0_fit1, col = "steelblue", lwd = 2)
  
# examine normality for zeta0 (school-level)
zeta0_fit1 <- blups_fit1$schoolid$`(Intercept)`
# density plot
plot(density(zeta0_fit1))
# QQ plot
qqnorm(zeta0_fit1, pch = 1, frame = FALSE, main = "Normal Q-Q plot for zeta0")
qqline(zeta0_fit1, col = "red", lwd = 2)
par(mfrow=c(1,1))  
```

QQ plot shows that both sets of BLUPs of zeta0 and eta0 are around the line, and thus we believe the normality assumption holds.

## Question 3

```{r question 3}
# a
# add a random slope for minority, correlated with the random intercept, at the school level
fit2 <- lmerTest::lmer(math1st ~ housepov + yearstea + mathprep + mathknow + ses + sex + minority + (minority|schoolid) + (1 | classid), data = dat)
print(summary(fit2))

# b
# residual
blups_fit2 <- ranef(fit2)
# BULPs
zeta0 <- blups_fit2$schoolid$`(Intercept)`
zeta1 <- blups_fit2$schoolid$minority
eta0 <- blups_fit2$classid$`(Intercept)`

# c
# check normality
par(mfrow=c(3,2))  
# density plot
plot(density(eta0))
# QQ plot
# examine normality for eta0 (class-level)
qqnorm(eta0, pch = 1, frame = FALSE, main = "Normal Q-Q plot for eta0")
qqline(eta0, col = "steelblue", lwd = 2)
  
# examine normality for zeta0 (school-level)
# density plot
plot(density(zeta0))
# QQ plot
qqnorm(zeta0, pch = 1, frame = FALSE, main = "Normal Q-Q plot for zeta0")
qqline(zeta0, col = "red", lwd = 2)

# examine normality for zeta1 (random slop)
# density plot
plot(density(zeta1))
# QQ plot
qqnorm(zeta1, pch = 1, frame = FALSE, main = "Normal Q-Q plot for zeta1")
qqline(zeta1, col = "chartreuse3", lwd = 2)
par(mfrow=c(1,1)) 
```
QQ plot shows that BLUPs of eta0 are around the line, and thus we believe the normality assumption holds.
However, BLUPs of zeta0, and zeta1 deviate from the line too much, and therefore we don't think the normality assumption holds.

```{r}
# d
plot(zeta1, zeta0)
```

Overall, zeta0 and zeta1 are negative correlated. However, some odd points are positive correlated.

```{r}
# e
# points from first quadrant
fq <- (3-abs(blups_fit2$schoolid$minority)>0)&(zeta1>=0&zeta0>=0)
# points from third quadrant
sq <- (3-abs(blups_fit2$schoolid$minority)>0)&(zeta1<=0&zeta0<=0)

# these schools are 
unique(dat$schoolid)[fq|sq]
odd_point <- 
  dat %>% 
  filter(schoolid %in% unique(dat$schoolid)[fq|sq])
```
Almost all students in these school are minority.

## Question 4
### a
$V_S = 169.45$, $V_C = 93.89$, and $V_E = 1064.96$.

```{r Question 4}
# fit a model
fit3 <- lmerTest::lmer(math1st ~ housepov + yearstea + mathprep + mathknow + ses + sex + minority + (ses|schoolid) + (1 | classid), 
                       data = dat)
summary(fit3)
```
### c
$V_C=86.57$, $V_{S(ses=0)}=171.18$, and $V_E=1035.90$.

### d
$V_{S(ses=-0.50)}=171.18+2*(-0.5)*13.083*8.565*0.19+(-0.5)^2*73.36=$ `r round(171.18+2*(-0.5)*13.083*8.565*0.19+(-0.5)^2*73.36, 2)`

$V_{S(ses=0.50)}=171.18+2*(0.5)*13.083*8.565*0.19+(0.5)^2*73.36=$ `r round(171.18+2*(0.5)*13.083*8.565*0.19+(0.5)^2*73.36, 2)`

### e
There is heteroscedasticity at school level (3) because $V_{S(ses=0.50)}$ and $V_{S(ses=-0.50)}$ are not approximate and $V_S$ are depend on ses.


# Project C – Longitudinal Data, Yu Wang

## 1. Make a person-period file with math score (Kindergarten and First grade). That is, math0 <- mathkind; math1 <- mathkind+mathgain (you have to make this work in the dataframe). Using reshape in R, you have to be careful to specify the name of the math variable (math0 and math1) as varying.
```{r}
library(tidyverse)
library(lme4)
library(lmerTest)
library(lattice)
require(foreign)
dat<- read.csv('classroom.csv')
dat$math0 <- dat$mathkind
dat$math1 <- dat$mathkind + dat$mathgain
class_pp <- reshape(dat, varying = c("math0", "math1"), v.names = "math", timevar = "year",
times = c(0, 1), direction = "long")

```




## 2.We ignore classrooms in this analysis, but keep it in the notation
### 2a. Fit a model with math as outcome, and fixed effect for time trend (year), and random intercepts for schools.
ANS:
```{r}
# a
fit1 <- lmer(math ~ year + (1 | schoolid), data = class_pp)
summary(fit1)
```

### 2b. Write down the model (include assumptions).
ANS :
$MATH_{tijk} = {b_0} + {\zeta_{0k}} + {b_1}TIME_{tijk} + {\varepsilon_{tijk}}$
and assume ${\zeta_{0k}} \sim N(0,\sigma_{{\zeta_0}}^2)$, and ${\varepsilon_{tijk}} \sim N(0,\sigma _\varepsilon ^2)$, independently.

### 2c. Add random intercepts for child
```{r}
fit2 <- lmer(math ~ year + (1 |schoolid/childid), data = class_pp)
summary(fit2)
```

### 2d. Write down the model (include assumptions).
ANS :
$MATH_{tijk} = {b_0} + {\delta_{0ijk}} + {\zeta_{0k}} + {b_1}TIME_{tijk} + {\varepsilon_{tijk}}$
and assume ${\delta_{0ijk}} \sim N(0,\sigma_{{\delta_0}}^2)$, ${\zeta_{0k}} \sim N(0,\sigma_{{\zeta_0}}^2)$, and ${\varepsilon_{tijk}} \sim N(0,\sigma _\varepsilon ^2)$, independently.




## 3. Report original and new variance estimates for $\sigma_{{\zeta_0}}^2$ (between schools) and $\sigma _\varepsilon ^2$(within schools)
ANS: The old between school variance is 348.7 where the old within school variance is 1268.4
     The new between school variance is 307.5 where the new within school variance is 599.1


### a Compute a pseudo R2 relating the between school variation and ignoring between students in the same school. In other words, what fraction of the between-school variance in the first model is ‘explained’ by the addition of a student random effect?
ANS: R^2 = (348.7-307.5)/348.7 = 0.11


### b. Does the total variation stay about the same (adding between children within schools variance as well, to the second model results)?
ANS: The total variance in the fit1 is 348.7 + 1268.4 = 1617.1
     The total variance in the fit2 is 702 + 307.5 + 599.1 = 1608.6
     Hence, the total variation seems stay about the same although their values are not exactly same.
     



## 4. Add a random slope (zeta1) for time trend (year) within schools (uncorrelated with random intercept (zeta0))
ANS:
```{r}
fit3 <- lmer(math ~ year + (year || schoolid) + (1 | schoolid:childid),data = class_pp)
summary(fit3)
```


### a Generate the BLUPs for the random effects and examine whether the independence between zeta0 and zeta1 is REFLECTED in a scatterplot of these two sets of effects.
ANS:
```{r}
# obtain the random effects
data <- ranef(fit3)

# plot the correlation
plot(data$schoolid$`(Intercept)`, data$schoolid$year)
```
Based on the above scatter plot, I would say zeta0 and zeta1 are independent because there does exist a patter which
can explain the relationship between year(x) and intercept(y)


### b. Compute V_S(year=0) and V_S(year=1). Since there are only two years, this is a form of heteroscedasticity in the random effects.
#### i. In which year is there more between school variation, net of all else (year=0 or year=1)?
ANS: V_S(year=0) = $\sigma_{{\zeta_0}}^2$ + 0 * $\sigma_{{\zeta_0}}^2$ = 324.79
     V_S(year=1) = $\sigma_{{\zeta_0}}^2$ + 1 * $\sigma_{{\zeta_0}}^2$ = 324.79 + 88.67 = 413.46
     Based on the hand calculation, I would say (year=1) has more between school variation 




## 5.If you ran the model separately BY YEAR, and removed the year trend from the model, would you get the same estimates for the variance between schools? TRY IT
ANS
```{r}
dat_temp = class_pp %>% filter(year==0)
fit_year0 = lmer(math ~ (1 | schoolid),data = dat_temp)
summary(fit_year0)

dat_temp2 = class_pp %>% filter(year==1)
fit_year1 = lmer(math ~ (1| schoolid), data = dat_temp2)
summary(fit_year1)
```
Based on the above summary table, we would say year1(364.3) has more between school variation than year0(306.8).




## 6. Rerun the last nested longitudinal model, allowing correlation between intercept and slope.
```{r}
fit4 <- lmer(math ~ year + (year | schoolid) + (1| schoolid:childid), data = class_pp)
summary(fit4)
```
### a. Is the correlation signif.?
```{r}
anova(fit3, fit4, refit = F)
```
ANS: Since the p value is much less than 0.05, we can conclude that the correlation is significant

### b.Compute V_S(year=0) and V_S(year=1) for this new model (your formula should include covariance terms).
ANS: 
covariance = -0.45 * 19.25 * 10.44 = -90.
V_S(Year=0) = 370.6 + 2 * 0 * (-90) + 0^2 * 109.1 = 370.6. 
V_S(Year=1) = 370.6 + 2 * 1 * (-90) +1^2 * 109.1 = 299.7.
     

## Is this result (and thus model) more consistent with the separate grade analysis? You are implicitly testing model fit here.
ANS: The calculated value for V_S(Year=0) is 370.6 where the actual between school variations 364.3. Also, the calculated value for V_S(Year=1) is 299.7 where the actual between school variance is about 306.6. As we can see, they are pretty similar, suggesting that the model is consistent with the separate grade analysis